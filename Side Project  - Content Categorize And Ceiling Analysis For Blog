# -*- coding: utf-8 -*-
"""Copy of Bản sao của Categorize content.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Gpie5bZgFi27x9CHozMWhd_X15sC9Fd

# Setup Enviroment
"""

from google.cloud import language_v1
from google.oauth2 import service_account
from google.cloud import translate_v2
import google.oauth2.credentials

import pandas as pd
from bs4 import BeautifulSoup
import json, csv, os

# To change later

key_json = {
    "type": "service_account",
    "project_id": "",
    "private_key_id": "",
    "private_key": ""
    "client_email": "",
    "client_id": "",
    "auth_uri": "https://accounts.google.com/o/oauth2/auth",
    "token_uri": "https://oauth2.googleapis.com/token",
    "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
    "client_x509_cert_url": "",
    "universe_domain": "googleapis.com"
}

with open('key.json', 'w') as f:
    json.dump(key_json, f)

# To change later

translate_key_json = {
    "type": "service_account",
    "project_id": "",
    "private_key_id": "",
    "private_key": "",
    "client_email": "",
    "client_id": "",
    "auth_uri": "https://accounts.google.com/o/oauth2/auth",
    "token_uri": "https://oauth2.googleapis.com/token",
    "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
    "client_x509_cert_url": ",
    "universe_domain": "googleapis.com"
}

with open('translate_key.json', 'w') as f:
    json.dump(translate_key_json, f)

creds = service_account.Credentials.from_service_account_file("key.json")
client = language_v1.LanguageServiceClient(credentials = creds,)

def entity_analytics(text_content, language = 'en'):
    type_ = language_v1.Document.Type.PLAIN_TEXT
    language = language
    document = {"content": text_content, "type_": type_, "language": language}
    encoding_type = language_v1.EncodingType.UTF8
    response = client.analyze_entities(request = {'document': document, 'encoding_type': encoding_type})
    entities_data = []
    entity_list = []

    for entity in response.entities:
        if entity.name not in entity_list:
            results = [
                entity.name,
                entity.type_.name,
                entity.salience,
                entity.metadata.get("wikipedia_url", "-"),
                entity.metadata.get("mid", "-"),
            ]

            entities_data.append(results)
            entity_list.append(entity.name)

    content_categories_version = (
        language_v1.ClassificationModelOptions.V2Model.ContentCategoriesVersion.V2
    )

    response = client.classify_text(request = {
        "document": document,
        "classification_model_options": {
            "v2_model": {
                "content_categories_version": content_categories_version
                }
            }
        }
    )

    category_data = []
    for category in response.categories:
        category_data.append([
            category.name,
            category.confidence
    ])

    output = {
        "entities_data": entities_data,
        "category_data": category_data
    }

    return output

creds = service_account.Credentials.from_service_account_file("translate_key.json")
translate_client = translate_v2.Client(credentials = creds)

def gg_translate(text_content, target_lang):
    if type(text_content) == str:
        text_content = [text_content]

    output = translate_client.translate(text_content, target_language = target_lang)

    return [item['translatedText'] for item in output]

import re
import requests

def scrape_html(url):
    try:
        response = requests.get(url)
        response.raise_for_status()

        return response.text

    except requests.RequestException as e:
        return None

def get_text_content(html_content):
    soup = BeautifulSoup(html_content, "html.parser")
    text_content = soup.get_text()

    text_content = re.sub(r'\n+', '\n', text_content)
    text_content = text_content.replace('\n', ' ')
    text_content = re.sub(r'\s{2,}', ' ', text_content)

    return text_content

def categorize_content(list_url_path, lang):
    print('Step 1: Start read list url')
    with open(list_url_path, 'r') as f:
      urls = [line for line in f.read().split('\n')]
    print('Total url is: ', len(urls))

    print('\nStep 2: Start craw content of list url')
    text_contents = []
    for url in urls:
        html_content = scrape_html(url)
        text_content = get_text_content(html_content)

        text_contents.append(text_content)

    print('\nStep 3: Start analytist your content')
    output = [['url', 'category', 'main_entities']]
    i = 0
    with open('output_data.csv', 'w') as f:
        write = csv.writer(f)
        write.writerow(['url','main_category', 'categories', 'main_entities'])
        for content in text_contents:
            try:
                print(i, urls[i])
                if lang == '':
                    content = gg_translate(content, 'en')[0]
                    data = entity_analytics(content, language = 'en')

                else:
                    data = entity_analytics(content, language = lang)

                cate = '\n'.join([':'.join([line[0], str(round(line[1] * 100) / 100)]) for line in data['category_data']])
                if len(data['category_data']) > 0:
                    main_category = data['category_data'][0][0]

                else:
                    main_category = []

                entities = []
                for item in data['entities_data']:
                    if item[0].lower() not in entities:
                        entities.append(item[0].lower())

                    if len(entities) > 5:
                        break

                tr_text = gg_translate(entities, 'id')
                entities = ','.join([f'{entities[j]} ({tr_text[j]})' for j in range(5)])
                output.append([urls[i], main_category, cate, entities])
                write.writerow([urls[i], main_category, cate, entities])

            except Exception as e:
                print('ERROR:', urls[i])
                print('ERROR:', e)

            i = i + 1

    print('\nFINISH PROCESS')
    print('Your data is saved at "/content/output_data.csv"')

    return output

"""# RUN"""

# LANG SUPPORT
# --------------------------------------
# Chinese (Simplified)                  zh
# Chinese (Traditional)	                zh-Hant
# English	                              en
# French	                              fr
# German	                              de
# Italian	                              it
# Japanese	                            ja
# Korean	                              ko
# Portuguese (Brazilian & Continental)	pt
# Russian	                              ru
# Spanish	                              es

# Set blank if your language not in list LANG SUPPORT
lang = ''

# should be txt file, 1 url perline
list_url_path = '/content/list_url.txt'

data = categorize_content(list_url_path, lang)

df =  pd.DataFrame(data, columns = ['url', 'category', 'Main Entities'])
df
